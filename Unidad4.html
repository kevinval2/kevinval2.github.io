<!sDOCTYPE html>
<html>

<head><title>UNIDAD 4</title></head>

<body>

<center><table width="20%" height="20%">
<tr><th>
<img src="Modelo.png" alt="modelo" height="100%" width="100%">
</th></tr>
</table>
</center>
<h1><hr size="10%" color="#ffffff" width="80%" align="center"></hr>
</h1>
<center>
<font color="#ffffff" face="Gill Sans MT" size=7><b>UNIDAD 4</b></font>

<h1><hr size="20%" color="#ffffff" width="100%" align="center"></hr>
</h1>
</center></h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=6><b>4.1 Aspectos Básicos de la Computación Paralela</b></font>
</center></h2>

<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y procesarlas de manera simultánea utilizando múltiples recursos de computación. Esto permite un procesamiento más rápido y eficiente en comparación con los enfoques secuenciales tradicionales. Algunos aspectos fundamentales de la computación paralela incluyen la sincronización de tareas, la comunicación entre procesos y la gestión de recursos.
</p></font></center></th></tr></table></center>

<center><table width="30%" height="30%">
<tr><th>
<div class="image-container">
            <td><img src="41.jpg" alt="modelo"></td>
        </div>
</th></tr>
</table>
</center>

<h1><hr size="20%" color="#ffffff" width="100%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=6><b>4.2 Tipos de Computación Paralela</b></font>
</center></h2>

<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios.<p>
Algunos de los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea.<p>
Estos enfoques se diferencian en cómo se dividen y procesan las tareas y los datos.
</p></font></center></th></tr></table></center>

<h1><hr size="10%" color="#ffffff" width="80%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=5><b>4.2.1 Clasificación</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
La clasificación de la computación paralela puede realizarse en función de la forma en que se dividen las tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos. Algunas clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea.
</p></font></center></th></tr></table></center>

<h1><hr size="10%" color="#ffffff" width="80%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=5><b>4.2.2 Arquitectura de Computadoras Secuenciales</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en los que las instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo común en muchas computadoras personales y estaciones de trabajo.
</p></font></center></th></tr></table></center>

<h1><hr size="10%" color="#ffffff" width="80%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=5><b>4.2.3 Organización de Direcciones de Memoria</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones de memoria en un sistema de computación paralela. Esto incluye consideraciones como la memoria compartida, la memoria distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en paralelo.
</p></font></center></th></tr></table></center>

<h1><hr size="20%" color="#ffffff" width="100%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=6><b>4.3 Sistemas de memoria(compartidas) Multiprocesadores</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples procesadores acceden a una misma área de memoria compartida. Esto permite a los procesadores compartir datos y comunicarse de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos tipos principales de redes: las redes de medio compartida y las redes conmutadas.
</p></font></center></th></tr></table></center>

<h1><hr size="10%" color="#ffffff" width="80%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=5><b>4.3.1  Redes de interconexión dinámica (Indirecta) Medio Compartido conmutadas</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
 Sistemas de memoria donde múltiples procesadores comparten el acceso
 a la memoria a través de redes de interconexión dinámicas.
</p></font></center></th></tr></table></center>

<h1><hr size="20%" color="#ffffff" width="100%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=6><b>4.4 Sistemas de memoria distribuida multicomputadoras</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
Los sistemas de memoria construida son una forma de organización de la memoria en la computación paralela en la que cada procesador tiene su propia memoria local. Esto permite una mayor independencia entre los procesadores y reduce la necesidad de acceder a una memoria compartida.
</p></font></center></th></tr></table></center>

<h1><hr size="20%" color="#ffffff" width="100%" align="center"></hr>
</h1>
<h2><center>
<font color="#ffffff" face="Gill Sans MT" size=6><b>4.5 Casos para estudio</b></font>
</center></h2>
<center><table width="80%" height="40%" class="center" bgcolor="#000000" border="25" bordercolor="#000000">
<tr><th>
<center><font color="#ffffff" face="Gill Sans MT" size=5><p align="justify">
En el campo de la computación paralela, existen numerosos casos de estudio que han demostrado la eficacia y los beneficios de los enfoques paralelos en diferentes dominios. Algunos ejemplos incluyen el uso de computación paralela en simulaciones científicas, análisis de grandes conjuntos de datos, renderizado de gráficos y modelado de sistemas complejos.
</p></font></center></th></tr></table></center>

<table width="30%" height="30%">
<tr><th>
<A href="Arquitectura de computadoras.html">
<img src="Regreso.png" alt="flecha" height="30%" width="15%">

</th></tr>
</table>

<body bgcolor="#FF0103">
</body>
</html>